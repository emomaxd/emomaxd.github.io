<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Effective TLB Usage — Keeping Address Translation Fast</title>
  <meta name="description" content="Advanced guide to avoiding TLB misses and page walks: levels, associativity, huge pages, access patterns, NUMA, virtualization, and Linux tuning." />
  <style>
    :root{
      --bg:#0b0f12;
      --panel:#0f1518;
      --muted:#95a0a6;
      --accent:#7bd389;
      --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, 'Roboto Mono', 'Courier New', monospace;
      --sans: Inter, ui-sans-serif, system-ui, -apple-system, 'Segoe UI', Roboto, 'Helvetica Neue';
    }
    *{box-sizing:border-box}
    body{
      margin:0;
      font-family:var(--sans);
      background:linear-gradient(180deg,var(--bg),#071013 120%);
      color:#e6eef3;
      line-height:1.6;
      padding:32px;
      max-width:820px;
      margin:auto;
    }
    h1,h2,h3{line-height:1.2}
    code{font-family:var(--mono);background:rgba(255,255,255,0.06);padding:2px 4px;border-radius:4px;font-size:90%}
    pre{
      background:#061014;
      padding:12px;
      border-radius:8px;
      border:1px solid rgba(255,255,255,0.05);
      overflow:auto;
      font-family:var(--mono);
      font-size:13px;
    }
    .meta{color:var(--muted);font-size:13px;margin-bottom:16px}
    a{color:var(--accent);text-decoration:none}
    a:hover{text-decoration:underline}
    .note{font-size:13px;color:var(--muted)}
    ul.tight>li{margin:6px 0}
  </style>
</head>
<body>
  <article>
    <h1>Effective TLB Usage: Keeping Address Translation Fast</h1>
    <div class="meta">Aug 13, 2025 · ~12 min read</div>

    <p>
      The Translation Lookaside Buffer (TLB) caches recent virtual→physical translations. A miss forces a hardware page walk, potentially hundreds of cycles if page tables aren't hot in cache. In modern workloads with large memory footprints, TLB behavior can become the dominant limiter even when caches are sufficient.
    </p>

    <h2>TLB architecture in practice</h2>
    <ul class="tight">
      <li><strong>Levels:</strong> Most x86-64 cores have small per-core L1 DTLB and ITLB, plus a larger shared STLB (second-level TLB). Miss in all levels ⇒ page walk.</li>
      <li><strong>Superpage TLBs:</strong> Many cores have dedicated entries for 2 MiB and 1 GiB pages, increasing reach without replacing 4 KiB entries.</li>
      <li><strong>Associativity:</strong> TLBs are set-associative. Conflict misses can happen if too many active pages map to the same set index.</li>
      <li><strong>Reach:</strong> TLB reach ≈ <em>entries × page size</em>. With 4 KiB pages, reach is tiny; with 2 MiB huge pages, it jumps 512× per entry.</li>
      <li><strong>PCID/ASID:</strong> Process Context Identifiers (PCID on x86, ASID on ARM) allow TLB entries from multiple address spaces to coexist, reducing flushes on context switch.</li>
      <li><strong>Prefetchers:</strong> Some µarchs speculatively prefetch TLB entries alongside data prefetch; effective when accesses are sequential.</li>
    </ul>

    <h2>When you hit the wall</h2>
    <ul class="tight">
      <li><strong>Large randomized working sets</strong> across thousands of small pages.</li>
      <li><strong>Pathological strides</strong> (≥ page size) → each access hits a new page, blowing through associativity.</li>
      <li><strong>Mixed code/data pressure</strong> — large JIT-generated code and large data arrays both consuming TLB entries.</li>
      <li><strong>Nested virtualization</strong> — Extended Page Tables (EPT) or Nested Page Tables (NPT) double the translation steps, making TLB miss cost higher.</li>
    </ul>

    <h2>Playbook: make the TLB your ally</h2>

    <h3>1) Increase reach with huge pages</h3>
    <p>
      Use 2 MiB or 1 GiB pages for large arrays/arenas. They cut translation entries drastically and reduce page walks.
    </p>
    <pre>// Transparent Huge Pages (THP) - easy path
madvise(ptr, len, MADV_HUGEPAGE);

// Explicit huge pages (requires pool setup)
void* p = mmap(NULL, len, PROT_READ|PROT_WRITE,
               MAP_PRIVATE|MAP_ANONYMOUS|MAP_HUGETLB, -1, 0);
    </pre>
    <p class="note">
      THP may be allocated lazily; MAP_HUGETLB is deterministic but needs preallocated pages (<code>echo 128 > /proc/sys/vm/nr_hugepages</code>).
    </p>

    <h3>2) Manage stride and layout</h3>
    <ul class="tight">
      <li>Favor contiguous scans and block accesses so a block spans few pages.</li>
      <li>SoA over AoS when touching a subset of fields — fewer pages visited.</li>
      <li>Avoid stride ≈ N×page_size patterns; they defeat caches and TLB.</li>
    </ul>

    <h3>3) Tile for TLB, not just cache</h3>
    <p>Classic cache tiling reduces capacity misses; add a <em>page-aware</em> dimension.</p>
    <pre>// Example: page-aware 2D blocking (row-major)
const int B = 256; // rows*elem_size*B ≈ a few pages
for (int i = 0; i < N; i += B)
  for (int j = 0; j < N; j += B)
    for (int ii = i; ii < min(i+B,N); ++ii)
      for (int jj = j; jj < min(j+B,N); ++jj)
        C[ii*N+jj] += A[ii*N+jj] * Bm[ii*N+jj];
    </pre>

    <h3>4) Warm translations</h3>
    <p>Touch each page once before tight loops to prefill TLB.</p>
    <pre>// Page-touch warmup (4 KiB pages)
for (size_t off = 0; off < bytes; off += 4096)
  asm volatile("" : : "r"(((volatile char*)buf)[off]) : "memory");
    </pre>

    <h3>5) Thread & NUMA awareness</h3>
    <ul class="tight">
      <li>Partition data so each thread works on disjoint pages.</li>
      <li>First-touch allocation binds memory to the local NUMA node.</li>
      <li>Batch map/unmap to reduce TLB shootdowns (IPIs).</li>
    </ul>

    <h3>6) Virtualization considerations</h3>
    <ul class="tight">
      <li>Nested paging doubles translation layers; huge pages help more.</li>
      <li>vTLB flushes can be costly — avoid frequent guest CR3 changes.</li>
    </ul>

    <h2>Microbenchmark: stride and huge page effect</h2>
    <p>
      This benchmark measures access time for varying strides with and without huge pages.
    </p>
    <pre>#define _GNU_SOURCE
#include &lt;stdint.h&gt;
#include &lt;stddef.h&gt;
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;string.h&gt;
#include &lt;sys/mman.h&gt;
#include &lt;x86intrin.h&gt;

static inline uint64_t rdtsc(){ unsigned aux; return __rdtscp(&aux); }

void run_test(uint64_t *a, size_t N, const char *label){
  size_t strides[] = {1,2,4,8,16,32,64,128,256,512,1024,2048,4096,8192};
  for(size_t s_i=0;s_i&lt;sizeof(strides)/sizeof(strides[0]);++s_i){
    size_t s = strides[s_i];
    for(size_t i=0;i&lt;N;i+=4096/sizeof(uint64_t))
      asm volatile(""::"r"(a[i]):"memory");
    uint64_t t0 = rdtsc(), acc=0;
    for(size_t i=0;i&lt;N;i+=s) acc += a[i];
    uint64_t t1 = rdtsc();
    printf("%s stride=%zu el (~%zu B): %llu cycles\n",
           label, s, s*sizeof(uint64_t),
           (unsigned long long)(t1-t0));
  }
}

int main(){
  const size_t N = (size_t)256*1024*1024 / sizeof(uint64_t); // 256 MiB
  uint64_t *normal;
  posix_memalign((void**)&normal, 4096, N*sizeof(uint64_t));
  for(size_t i=0;i&lt;N;i++) normal[i]=i;
  run_test(normal, N, "4K");

  void *huge = mmap(NULL, N*sizeof(uint64_t),
    PROT_READ|PROT_WRITE,
    MAP_PRIVATE|MAP_ANONYMOUS|MAP_HUGETLB, -1, 0);
  if (huge != MAP_FAILED) {
    uint64_t *hp = (uint64_t*)huge;
    for(size_t i=0;i&lt;N;i++) hp[i]=i;
    run_test(hp, N, "2M");
    munmap(hp, N*sizeof(uint64_t));
  } else {
    fprintf(stderr, "Hugepage alloc failed; run with preallocated pool.\n");
  }
  free(normal);
  return 0;
}
    </pre>

    <h2>Observability</h2>
    <p>Use <code>perf</code> or hardware counters to measure TLB pressure:</p>
    <pre># Data TLB
perf stat -e dTLB-load-misses,dTLB-store-misses ./a.out

# Instruction TLB
perf stat -e iTLB-load-misses ./a.out

# Page walk cycles
perf stat -e dtlb_walk_pending,itlb_walk_pending ./a.out

# PCID flush count (if available)
perf stat -e tlb_flush ./a.out
    </pre>

    <h2>Checklist</h2>
    <ul class="tight">
      <li>Measure before/after changes: dTLB/iTLB misses, walk cycles.</li>
      <li>Coalesce allocations into contiguous arenas.</li>
      <li>Adopt huge pages for large hot data.</li>
      <li>Block for TLB (avoid page-sized strides).</li>
      <li>Pin threads & allocate local to NUMA node.</li>
      <li>Minimize TLB shootdowns — batch mapping changes.</li>
      <li>Consider virtualization costs; huge pages reduce them.</li>
    </ul>

    <p><a href="../blog.html">← Back to blog</a></p>
  </article>
</body>
</html>

