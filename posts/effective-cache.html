<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Effective Cache Usage — Reducing Misses & Improving Locality</title>
  <style>
    body { font-family: sans-serif; max-width: 800px; margin: auto; padding: 20px; line-height: 1.6; background: #0b0f14; color: #ddd; }
    h1, h2, h3 { color: #fff; }
    code { background: #111; padding: 2px 4px; border-radius: 4px; color: #0ff; }
    pre { background: #111; padding: 10px; overflow-x: auto; border-radius: 6px; }
    a { color: #0ff; text-decoration: none; }
    a:hover { text-decoration: underline; }
  </style>
</head>
<body>
  <article>
    <h1>Effective Cache Usage: Reducing Misses & Improving Locality</h1>
    <p><em>Published: Aug 13, 2025 · ~9 min read</em></p>

    <p>Modern CPUs can execute instructions much faster than they can fetch data from main memory. This performance gap is bridged by multi-level caches (L1, L2, L3), but poor memory access patterns can cause frequent cache misses, stalling the pipeline.</p>

    <h2>Cache Hierarchy & Latency</h2>
    <p>Typical access latencies (x86-64):</p>
    <ul>
      <li><strong>L1 Cache:</strong> ~4 cycles (~1ns)</li>
      <li><strong>L2 Cache:</strong> ~12 cycles (~3ns)</li>
      <li><strong>L3 Cache:</strong> ~40 cycles (~10ns)</li>
      <li><strong>DRAM:</strong> 100+ cycles (~60–100ns)</li>
    </ul>
    <p>Any access missing all cache levels forces a DRAM fetch — hundreds of wasted cycles.</p>

    <h2>Locality Principles</h2>
    <ul>
      <li><strong>Spatial locality:</strong> Access memory locations close to each other in address space (arrays are better than linked lists).</li>
      <li><strong>Temporal locality:</strong> Reuse recently accessed data before it gets evicted.</li>
    </ul>

    <h2>Practical Optimizations</h2>
    <h3>1. Structure-of-Arrays (SoA) over Array-of-Structures (AoS)</h3>
    <pre><code>// Bad: AoS — causes false spatial locality
struct Point { float x, y, z; };
Point pts[N];  // Accessing only x still loads y and z

// Good: SoA — tight packing of used fields
float xs[N], ys[N], zs[N];
</code></pre>

    <h3>2. Blocking / Tiling</h3>
    <p>When processing large datasets (matrices, grids), work on small blocks that fit entirely into L1/L2 cache.</p>
    <pre><code>for (int i = 0; i &lt; N; i += block) {
  for (int j = 0; j &lt; N; j += block) {
    for (int ii = i; ii &lt; i + block; ++ii) {
      for (int jj = j; jj &lt; j + block; ++jj) {
        C[ii][jj] = A[ii][jj] + B[ii][jj];
      }
    }
  }
}
</code></pre>

    <h3>3. Prefetching</h3>
    <p>Use hardware prefetch by accessing predictable addresses. For unpredictable access, consider <code>_mm_prefetch()</code> intrinsics.</p>

    <h2>Measuring Cache Performance</h2>
    <p>On Linux, use <code>perf stat</code> to see cache misses:</p>
    <pre><code>perf stat -e cache-references,cache-misses ./a.out</code></pre>

    <h2>Summary</h2>
    <ul>
      <li>Keep working sets small enough to fit in L1/L2 when possible.</li>
      <li>Maximize spatial and temporal locality.</li>
      <li>Use SoA layout for vectorized and cache-friendly access.</li>
      <li>Leverage blocking, tiling, and prefetching for large datasets.</li>
    </ul>

    <p><a href="../blog.html">← Back to blog</a></p>
  </article>
</body>
</html>

